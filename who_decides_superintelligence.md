

## Reflections on "Preparing for the Intelligence Explosion" (MacAskill & Moorhouse, 2025)

### The Gap in the Paper

The paper provides an excellent catalog of grand challenges but leaves a critical gap: **who decides, and how do we select them?**

The paper says "empower responsible actors" but offers only vague mechanisms (ML community voting, VC selection). These are inadequate—both are deeply entangled in existing interests.

### The Selection Problem

Without a rigorous selection process, we get:
- A false binary: "stop AI" (0) vs "race to win" (100)
- Each AI company thinking "since no one will build a proper process, we must win"
- Self-fulfilling prophecy toward dangerous competition

### Candidate Criteria (for whoever holds superintelligence)

**Required:**
- Empathy and responsibility at humanity-scale, not just family/organization
- Seriousness about vision
- Ability to detect drift/misalignment
- Not extreme, but not paralyzed by "balance"
- Won't blindly defer to AI

**Exclusions:**
- Those who would stagnate progress
- Those easily manipulated
- Those seeking power for its own sake

**Critical addition:** Candidates must be demonstrably too intelligent to manipulate. This discourages bad actors from infiltrating the process.

### Evaluator Criteria

- Understands the value of diverse worldviews
- Doesn't consider their own worldview superior
- Can collaborate across worldviews
- Has empathy for future humanity
- Has no stake in the outcome

### Structural Protections

**For candidates:**
- Physical safety
- Psychological safety (to prevent paranoia)
- Economic independence (can't be leveraged)
- Guarantee of safety even if they refuse to be puppets

**For the process:**
- Active search, not open applications (prevents infiltration)
- Behavior-based evaluation over self-reporting
- Evaluators cannot become candidates

### The Incentive Problem

No existing organization has incentive to build this:
- AI companies: would lose their position
- Governments: don't understand the stakes
- International bodies: too slow
- Academia: no power

Current implicit logic: "No one will do proper selection → we must win → safety is secondary → but we'll say we're responsible"

### The Military Shortcut Risk

Rational (but dangerous) strategy: Don't build full superintelligence first. Build military-dominant AI first, suppress competitors, then complete superintelligence at leisure.

This makes the selection problem urgent—before this path becomes the default.

### A Possible Path Forward

1. An organization achieves defensive + counter-capability superiority
2. They can't be attacked, but don't need to attack
3. From this secure position, they propose selection process
4. Others agree because "we didn't lose, competition is pointless now"
5. Selection proceeds without winner-take-all dynamics

### The Deeper Principle

Rather than addressing challenges individually, **design structures where bad behavior is structurally unprofitable:**

- Military: Counter-capability makes attacks costly
- Economic: Exploitation less profitable than cooperation  
- Political: Power concentration has high maintenance costs
- Social: Reputation systems make defection costly

Layer these. If one fails, others hold.

### Conclusion

The paper's "century in a decade" framing is valuable. But without solving "who decides," we default to competition, which defaults to whoever rushes military AI fastest.

The selection process isn't optional—it's the missing piece that determines whether any other preparation matters.

-
